{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emilv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emilv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define file path and chunk size\n",
    "file_path = \"995,000_rows.csv\"\n",
    "chunksize = 40000\n",
    "\n",
    "# Define your cleaning function\n",
    "def clean_text(data):\n",
    "    if not isinstance(data, str):  # Handle NaN values safely\n",
    "        return \"\"\n",
    "    data = data.lower()\n",
    "    data = re.sub(r'\\s+', \" \", data)\n",
    "    data = re.sub(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', \"<DATE>\", data)\n",
    "    data = re.sub(r'(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec).? \\d{1,2},? \\d{4}', \"<DATE>\", data)\n",
    "    data = re.sub(r'\\d{4}-\\d{2}-\\d{2}', \"<DATE>\", data)\n",
    "    data = re.sub(r'[\\w._%+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', \"<EMAIL>\", data)\n",
    "    data = re.sub(r'http[s]?://[^\\s]+', \"<URL>\", data)\n",
    "    data = re.sub(r'\\d+(\\.\\d+)?', \"<NUM>\", data)\n",
    "    return data\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Define function to tokenize, remove stopwords, and stem\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [ps.stem(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# List of columns to process\n",
    "columns_to_clean = [\n",
    "    \"Unamed: 0\",\n",
    "    \"id\", \"domain\", \"type\", \"url\", \"content\", \"scraped_at\", \"inserted_at\",\n",
    "    \"updated_at\", \"title\", \"authors\", \"keywords\", \"meta_keywords\",\n",
    "    \"meta_description\", \"tags\", \"summary\"\n",
    "]\n",
    "\n",
    "#Process and store all chunks in a single DataFrame\n",
    "preprocessed_data = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize, low_memory=False):\n",
    "    for col in columns_to_clean:\n",
    "        if col in chunk.columns:\n",
    "            # Clean\n",
    "            chunk['content'] = chunk['content'].apply(clean_text)\n",
    "            #Remove stopwords\n",
    "            chunk['content'] = chunk['content'].astype(str).apply(\n",
    "                lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words])\n",
    "            )\n",
    "            #Tokenize and stem\n",
    "            chunk['content'] = chunk['content'].apply(tokenize_and_stem)\n",
    "    # Append chunk to final DataFrame\n",
    "    preprocessed_data = pd.concat([preprocessed_data, chunk], ignore_index=True)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "preprocessed_data.to_csv(\"cleaned_file.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows processed: 995000\n",
      "\n",
      "Missing values in metadata columns (count):\n",
      "authors             442757\n",
      "meta_keywords        38790\n",
      "meta_description    525106\n",
      "tags                764081\n",
      "summary             995000\n",
      "dtype: int64\n",
      "\n",
      "Missing values in metadata columns (percentage):\n",
      "authors              44.50\n",
      "meta_keywords         3.90\n",
      "meta_description     52.77\n",
      "tags                 76.79\n",
      "summary             100.00\n",
      "dtype: float64\n",
      "\n",
      "Domain distribution (top 10):\n",
      "nytimes.com           176144\n",
      "beforeitsnews.com      91468\n",
      "dailykos.com           77640\n",
      "express.co.uk          55983\n",
      "nationalreview.com     37377\n",
      "sputniknews.com        37229\n",
      "abovetopsecret.com     27947\n",
      "wikileaks.org          23699\n",
      "www.newsmax.com        12688\n",
      "www.ammoland.com       11129\n",
      "dtype: int64\n",
      "\n",
      "Total articles with 'error' in content: 0\n",
      "\n",
      "Content length statistics:\n",
      "count    995000.0\n",
      "mean          2.0\n",
      "std           0.0\n",
      "min           2.0\n",
      "25%           2.0\n",
      "50%           2.0\n",
      "75%           2.0\n",
      "max           2.0\n",
      "dtype: float64\n",
      "\n",
      "Total URLs found in content: 0\n",
      "Total dates found in content: 0\n",
      "Total numeric values found in content: 0\n",
      "\n",
      "Top 100 most frequent words:\n",
      "Not enough words to plot the top 10,000 frequencies.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cleaned_file = \"cleaned_file.csv\"\n",
    "chunksize = 25000  \n",
    "\n",
    "# Columns to analyze for missing metadata\n",
    "metadata_cols = ['authors', 'meta_keywords', 'meta_description', 'tags', 'summary']\n",
    "\n",
    "# Accumulators for initial observations\n",
    "total_rows = 0\n",
    "missing_counts_acc = None\n",
    "domain_counts_acc = {}\n",
    "error_count_acc = 0\n",
    "content_lengths = []\n",
    "\n",
    "# Accumulators for content analysis\n",
    "total_urls = 0\n",
    "total_dates = 0\n",
    "total_numerics = 0\n",
    "word_counter = Counter()\n",
    "\n",
    "# Process the CSV in chunks\n",
    "for chunk in pd.read_csv(cleaned_file, chunksize=chunksize, low_memory=False):\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Observation 1: Missing values for metadata columns \n",
    "    chunk_missing = chunk[metadata_cols].isnull().sum()\n",
    "    if missing_counts_acc is None:\n",
    "        missing_counts_acc = chunk_missing\n",
    "    else:\n",
    "        missing_counts_acc += chunk_missing\n",
    "\n",
    "    # Observation 2: Domain distribution \n",
    "    chunk_domain_counts = chunk['domain'].value_counts()\n",
    "    for domain, count in chunk_domain_counts.items():\n",
    "        domain_counts_acc[domain] = domain_counts_acc.get(domain, 0) + count\n",
    "\n",
    "    # Observation 3: Content Artifacts and Anomalies\n",
    "    # Convert the 'content' column to string\n",
    "    chunk['content'] = chunk['content'].astype(str)\n",
    "    # Detect rows containing \"error\"\n",
    "    error_mask = chunk['content'].str.contains(r\"\\berror\\b\", case=False, regex=True, na=False)\n",
    "    error_count_acc += error_mask.sum()\n",
    "\n",
    "    # Accumulate content lengths\n",
    "    content_lengths.extend(chunk['content'].apply(len).tolist())\n",
    "    \n",
    "    # Counting URLs, dates, numeric values, and aggregating words\n",
    "    # Inline regex patterns with (?i) for case-insensitive matching\n",
    "    url_regex = r'(?i)https?://\\S+'\n",
    "    date_regex = r'(?i)\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}-\\d{2}-\\d{2})\\b'\n",
    "    numeric_regex = r'\\b\\d+(?:\\.\\d+)?\\b'\n",
    "    word_regex = r'\\w+'\n",
    "    \n",
    "    total_urls += chunk['content'].str.count(url_regex).sum()\n",
    "    total_dates += chunk['content'].str.count(date_regex).sum()\n",
    "    total_numerics += chunk['content'].str.count(numeric_regex).sum()\n",
    "    \n",
    "    # Aggregate word counts for the top frequent words\n",
    "    combined_text = \" \".join(chunk['content'].tolist())\n",
    "    words = re.findall(word_regex, combined_text.lower())\n",
    "    word_counter.update(words)\n",
    "\n",
    "# Results after processing\n",
    "print(\"Total rows processed:\", total_rows)\n",
    "\n",
    "print(\"\\nMissing values in metadata columns (count):\")\n",
    "print(missing_counts_acc)\n",
    "print(\"\\nMissing values in metadata columns (percentage):\")\n",
    "print((missing_counts_acc / total_rows * 100).round(2))\n",
    "\n",
    "print(\"\\nDomain distribution (top 10):\")\n",
    "domain_series = pd.Series(domain_counts_acc).sort_values(ascending=False)\n",
    "print(domain_series.head(10))\n",
    "\n",
    "print(\"\\nTotal articles with 'error' in content:\", error_count_acc)\n",
    "\n",
    "print(\"\\nContent length statistics:\")\n",
    "print(pd.Series(content_lengths).describe())\n",
    "\n",
    "# Additional content analysis results\n",
    "print(\"\\nTotal URLs found in content:\", total_urls)\n",
    "print(\"Total dates found in content:\", total_dates)\n",
    "print(\"Total numeric values found in content:\", total_numerics)\n",
    "\n",
    "# Top 100 most frequent words\n",
    "top_100_words = word_counter.most_common(100)\n",
    "print(\"\\nTop 100 most frequent words:\")\n",
    "for word, count in top_100_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Plot frequency of the top 10,000 most frequent words\n",
    "top_10000 = word_counter.most_common(10000)\n",
    "if top_10000:\n",
    "    ranks = range(1, len(top_10000) + 1)\n",
    "    frequencies = [freq for word, freq in top_10000]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ranks, frequencies)\n",
    "    plt.xlabel('Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Frequency of the Top 10,000 Words')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Not enough words to plot the top 10,000 frequencies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emilv\\AppData\\Local\\Temp\\ipykernel_5860\\1538483547.py:2: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  split_cleaned_file = pd.read_csv(\"måske_cleaned_file.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: 796000\n",
      "Validation data shape: 99500\n",
      "Testing data shape: 99500\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned file\n",
    "split_cleaned_file = pd.read_csv(\"cleaned_file.csv\")\n",
    "#Split dataset in 80% train and 10% test and 10% validation\n",
    "training_data = split_cleaned_file.head(int(len(split_cleaned_file)*0.8))\n",
    "remaining_data = split_cleaned_file.tail(int(len(split_cleaned_file)*0.2))\n",
    "testing_data = remaining_data.head(int(len(split_cleaned_file)*0.1))\n",
    "validation_data = remaining_data.tail(int(len(split_cleaned_file)*0.1))\n",
    "\n",
    "\n",
    "#Print the shapes of the resulting datasets\n",
    "print(\"Training data shape:\", len(training_data))\n",
    "print(\"Validation data shape:\", len(validation_data))\n",
    "print(\"Testing data shape:\", len(testing_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
