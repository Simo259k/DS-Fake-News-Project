{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91d37af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/simonhvidtfeldt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   id                domain        type  \\\n",
      "0           0  141               awm.com  unreliable   \n",
      "1           1  256     beforeitsnews.com        fake   \n",
      "2           2  700           cnnnext.com  unreliable   \n",
      "3           3  768               awm.com  unreliable   \n",
      "4           4  791  bipartisanreport.com   clickbait   \n",
      "\n",
      "                                                 url  \\\n",
      "0  http://awm.com/church-congregation-brings-gift...   \n",
      "1  http://beforeitsnews.com/awakening-start-here/...   \n",
      "2  http://www.cnnnext.com/video/18526/never-hike-...   \n",
      "3  http://awm.com/elusive-alien-of-the-sea-caught...   \n",
      "4  http://bipartisanreport.com/2018/01/21/trumps-...   \n",
      "\n",
      "                                             content  \\\n",
      "0  sometimes the power of christmas will make you...   \n",
      "1  awakening of <NUM> strands of dna – “reconnect...   \n",
      "2  never hike alone: a friday the <NUM>th fan fil...   \n",
      "3  when a rare shark was caught, scientists were ...   \n",
      "4  donald trump has the unnerving ability to abil...   \n",
      "\n",
      "                   scraped_at                 inserted_at  \\\n",
      "0  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "1  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "2  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "3  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "4  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "\n",
      "                   updated_at  \\\n",
      "0  2018-02-02 01:19:41.756664   \n",
      "1  2018-02-02 01:19:41.756664   \n",
      "2  2018-02-02 01:19:41.756664   \n",
      "3  2018-02-02 01:19:41.756664   \n",
      "4  2018-02-02 01:19:41.756664   \n",
      "\n",
      "                                               title          authors  \\\n",
      "0  church congregation brings gift to waitresses ...      ruth harris   \n",
      "1  awakening of <NUM> strands of dna – “reconnect...     zurich times   \n",
      "2  never hike alone - a friday the <NUM>th fan fi...              nan   \n",
      "3  elusive ‘alien of the sea ‘ caught by scientis...  alexander smith   \n",
      "4  trump’s genius poll is complete & the results ...  gloria christie   \n",
      "\n",
      "  keywords meta_keywords                                   meta_description  \\\n",
      "0      nan          ['']                                                nan   \n",
      "1      nan          ['']                                                nan   \n",
      "2      nan          ['']  never hike alone: a friday the <NUM>th fan fil...   \n",
      "3      nan          ['']                                                nan   \n",
      "4      nan          ['']                                                nan   \n",
      "\n",
      "  tags summary  \n",
      "0  nan     nan  \n",
      "1  nan     nan  \n",
      "2  nan     nan  \n",
      "3  nan     nan  \n",
      "4  nan     nan  \n",
      "\n",
      "===== FULL TEXT (from DataFrame) =====\n",
      "\n",
      "sometimes the power of christmas will make you do wild and wonderful things. you do not need to believe in the holy trinity to believe in the positive power of doing good for others. the simple act of giving without receiving is lost on many of us these days, as worries about money and success hold us back from giving to others who are in need. one congregation in ohio was moved to action by the power of a sermon given at their church on christmas eve. the pastor at grand lake united methodist church in celina, ohio gave an emotional sermon about the importance of understanding the message of jesus. for many religious people the message of jesus is to help others before yourself, to make sure the people who are suffering get the help they need to enjoy life a little bit. the sermon was really about generosity and what that can look like in our lives. jesus lived a long time ago and he acted generously in the fashion of his time – but what would a generous act look like in our times? th\n",
      "\n",
      "===== CLEANED RAW FILE TEXT =====\n",
      "\n",
      ",id,domain,type,url,content,scraped_at,inserted_at,updated_at,title,authors,keywords,meta_keywords,meta_description,tags,summary <NUM>,<NUM>,awm.com,unreliable,<URL> the power of christmas will make you do wild and wonderful things. you do not need to believe in the holy trinity to believe in the positive power of doing good for others. the simple act of giving without receiving is lost on many of us these days, as worries about money and success hold us back from giving to others who are in need. one congregation in ohio was moved to action by the power of a sermon given at their church on christmas eve. the pastor at grand lake united methodist church in celina, ohio gave an emotional sermon about the importance of understanding the message of jesus. for many religious people the message of jesus is to help others before yourself, to make sure the people who are suffering get the help they need to enjoy life a little bit. the sermon was really about generosity and what that can look \n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/simonhvidtfeldt/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_text[:\u001b[38;5;241m1000\u001b[39m])  \u001b[38;5;66;03m# Print first 1000 characters\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Tokenize the cleaned text\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m tokens_cleaned \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m tokens_full \u001b[38;5;241m=\u001b[39m word_tokenize(full_text)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Print token samples\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/simonhvidtfeldt/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure nltk punkt tokenizer is downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load CSV (Ensure correct column names)\n",
    "file_path = \"news_sample.csv\"\n",
    "textpd = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Define the clean_text function\n",
    "def clean_text(data):\n",
    "    if not isinstance(data, str):  # Handle NaN values safely\n",
    "        return \"\"\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    data = data.lower()\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    data = re.sub(r'\\s+', \" \", data)\n",
    "\n",
    "    # Replace dates\n",
    "    data = re.sub(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', \"<DATE>\", data)\n",
    "    data = re.sub(r'(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec).? \\d{1,2},? \\d{4}', \"<DATE>\", data)\n",
    "    data = re.sub(r'\\d{4}-\\d{2}-\\d{2}', \"<DATE>\", data)\n",
    "\n",
    "    # Replace emails\n",
    "    data = re.sub(r'[\\w._%+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', \"<EMAIL>\", data)\n",
    "\n",
    "    # Replace URLs\n",
    "    data = re.sub(r'http[s]?://[^\\s]+', \"<URL>\", data)\n",
    "\n",
    "    # Replace numbers\n",
    "    data = re.sub(r'\\d+(\\.\\d+)?', \"<NUM>\", data)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Define relevant text columns\n",
    "columns_to_clean = [\"content\", \"title\", \"authors\", \"tags\", \"summary\", \"meta_description\", \"keywords\", \"meta_keywords\"]\n",
    "\n",
    "# Apply cleaning to each column\n",
    "for col in columns_to_clean:\n",
    "    if col in textpd.columns:  # Avoid KeyError if column is missing\n",
    "        textpd[col] = textpd[col].astype(str).apply(clean_text)\n",
    "\n",
    "# Print DataFrame preview after cleaning\n",
    "print(textpd.head())\n",
    "\n",
    "# Clean the raw file text separately\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "# Combine all cleaned text from DataFrame columns\n",
    "full_text = \" \".join(textpd[col].dropna().astype(str).str.cat(sep=\" \") for col in columns_to_clean if col in textpd.columns)\n",
    "\n",
    "# Print sample of cleaned text\n",
    "print(\"\\n===== FULL TEXT (from DataFrame) =====\\n\")\n",
    "print(full_text[:1000])  # Print first 1000 characters\n",
    "\n",
    "print(\"\\n===== CLEANED RAW FILE TEXT =====\\n\")\n",
    "print(cleaned_text[:1000])  # Print first 1000 characters\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "tokens_cleaned = word_tokenize(cleaned_text)\n",
    "tokens_full = word_tokenize(full_text)\n",
    "\n",
    "# Print token samples\n",
    "print(\"\\n===== TOKENIZED CLEANED TEXT SAMPLE =====\\n\", tokens_cleaned[:50])\n",
    "print(\"\\n===== TOKENIZED FULL TEXT SAMPLE =====\\n\", tokens_full[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d14e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def virker_lortet():\n",
    "    print(\"Lortet virker\")\n",
    "\n",
    "virker_lortet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4246980a-1993-4e33-8427-47cecafd0b26",
   "metadata": {},
   "source": [
    "For better comparison has frequency analysis been done for both my own cleaning function and the clean-text module function. The results from these functions are very similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04553b89-ab8a-4e9f-bd33-0e65554a78ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
