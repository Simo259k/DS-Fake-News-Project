{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define file path and chunk size\n",
    "file_path = \"995,000_rows.csv\"\n",
    "chunksize = 25000\n",
    "\n",
    "# Define your cleaning function\n",
    "def clean_text(data):\n",
    "    if not isinstance(data, str):  # Handle NaN values safely\n",
    "        return \"\"\n",
    "    data = data.lower()\n",
    "    data = re.sub(r'\\s+', \" \", data)\n",
    "    data = re.sub(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', \"<DATE>\", data)\n",
    "    data = re.sub(r'(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec).? \\d{1,2},? \\d{4}', \"<DATE>\", data)\n",
    "    data = re.sub(r'\\d{4}-\\d{2}-\\d{2}', \"<DATE>\", data)\n",
    "    data = re.sub(r'[\\w._%+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', \"<EMAIL>\", data)\n",
    "    data = re.sub(r'http[s]?://[^\\s]+', \"<URL>\", data)\n",
    "    data = re.sub(r'\\d+(\\.\\d+)?', \"<NUM>\", data)\n",
    "    return data\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Define function to tokenize, remove stopwords, and stem\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [ps.stem(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# List of columns to process\n",
    "columns_to_clean = [\n",
    "    \"id\", \"domain\", \"type\", \"url\", \"content\", \"scraped_at\", \"inserted_at\",\n",
    "    \"updated_at\", \"title\", \"authors\", \"keywords\", \"meta_keywords\",\n",
    "    \"meta_description\", \"tags\", \"summary\"\n",
    "]\n",
    "\n",
    "# Option 1: Process and store all chunks in a single DataFrame\n",
    "preprocessed_data = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize, low_memory=False):\n",
    "    for col in columns_to_clean:\n",
    "        if col in chunk.columns:\n",
    "            # Clean the column\n",
    "            chunk[col] = chunk[col].apply(clean_text)\n",
    "            # Create new column with tokenized and stemmed tokens\n",
    "            token_col = col + \"_tokens\"\n",
    "            chunk[token_col] = chunk[col].apply(tokenize_and_stem)\n",
    "    preprocessed_data = pd.concat([preprocessed_data, chunk], ignore_index=True)\n",
    "\n",
    "# Save the final DataFrame to a CSV file if desired\n",
    "preprocessed_data.to_csv(\"preprocessed_data_option1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emilv\\AppData\\Local\\Temp\\ipykernel_15956\\1659952360.py:9: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file = pd.read_csv(file_path)\n",
      "C:\\Users\\emilv\\AppData\\Local\\Temp\\ipykernel_15956\\1659952360.py:12: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(Final_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812913\n",
      "805448\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "file_path = \"995,000_rows.csv\"\n",
    "file = pd.read_csv(file_path)\n",
    "\n",
    "Final_file_path = \"preprocessed_data_option1.csv\"\n",
    "data = pd.read_csv(Final_file_path)\n",
    "\n",
    "#Check Unique values\n",
    "print(len(file['content'].unique()))\n",
    "print(len(data['content'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows processed: 995000\n",
      "\n",
      "Missing values in metadata columns (count):\n",
      "authors             442757\n",
      "meta_keywords        38790\n",
      "meta_description    525106\n",
      "tags                764081\n",
      "summary             995000\n",
      "dtype: int64\n",
      "\n",
      "Missing values in metadata columns (percentage):\n",
      "authors              44.50\n",
      "meta_keywords         3.90\n",
      "meta_description     52.77\n",
      "tags                 76.79\n",
      "summary             100.00\n",
      "dtype: float64\n",
      "\n",
      "Domain distribution (top 10):\n",
      "nytimes.com           176144\n",
      "beforeitsnews.com      91468\n",
      "dailykos.com           77640\n",
      "express.co.uk          55983\n",
      "nationalreview.com     37377\n",
      "sputniknews.com        37229\n",
      "abovetopsecret.com     27947\n",
      "wikileaks.org          23699\n",
      "www.newsmax.com        12688\n",
      "www.ammoland.com       11129\n",
      "dtype: int64\n",
      "\n",
      "Total articles with explicity 'error' in content: 78554\n"
     ]
    }
   ],
   "source": [
    "chunksize = 10000 \n",
    "\n",
    "# Columns to analyze for missing metadata\n",
    "metadata_cols = ['authors', 'meta_keywords', 'meta_description', 'tags', 'summary']\n",
    "\n",
    "# Accumulators\n",
    "total_rows = 0\n",
    "missing_counts_acc = None\n",
    "domain_counts_acc = {}\n",
    "error_count_acc = 0\n",
    "content_lengths = []\n",
    "\n",
    "# Process the CSV in chunks\n",
    "for chunk in pd.read_csv(Final_file_path, chunksize=chunksize, low_memory=False):\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Observation 1: Missing values for metadata columns\n",
    "    chunk_missing = chunk[metadata_cols].isnull().sum()\n",
    "    if missing_counts_acc is None:\n",
    "        missing_counts_acc = chunk_missing\n",
    "    else:\n",
    "        missing_counts_acc += chunk_missing\n",
    "\n",
    "    # Observation 2: Domain distribution \n",
    "    chunk_domain_counts = chunk['domain'].value_counts()\n",
    "    for domain, count in chunk_domain_counts.items():\n",
    "        domain_counts_acc[domain] = domain_counts_acc.get(domain, 0) + count\n",
    "\n",
    "    # Observation 3: Content Artifacts and Anomalies\n",
    "    # Convert the 'content' column to string\n",
    "    chunk['content'] = chunk['content'].astype(str)\n",
    "    # Detect rows containing explicitly \"error\"\n",
    "    error_mask = chunk['content'].str.contains(r\"\\berror\\b\", case=False, regex=True, na=False)\n",
    "    error_count_acc += error_mask.sum()\n",
    "\n",
    "\n",
    "# Results after processing\n",
    "print(\"Total rows processed:\", total_rows)\n",
    "\n",
    "print(\"\\nMissing values in metadata columns (count):\")\n",
    "print(missing_counts_acc)\n",
    "print(\"\\nMissing values in metadata columns (percentage):\")\n",
    "print((missing_counts_acc / total_rows * 100).round(2))\n",
    "\n",
    "print(\"\\nDomain distribution (top 10):\")\n",
    "domain_series = pd.Series(domain_counts_acc).sort_values(ascending=False)\n",
    "print(domain_series.head(10))\n",
    "\n",
    "print(\"\\nTotal articles with explicity 'error' in content:\", error_count_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: 796000\n",
      "Validation data shape: 99500\n",
      "Testing data shape: 99500\n"
     ]
    }
   ],
   "source": [
    "#Split dataset in 80% train and 10% test and 10% validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_data = data.sample(frac=0.8, random_state=42)\n",
    "remaining_data = data.drop(training_data.index)\n",
    "validation_data = remaining_data.sample(frac=0.5, random_state=42)\n",
    "testing_data = remaining_data.drop(validation_data.index)\n",
    "\n",
    "print(\"Training data shape:\", len(training_data))\n",
    "print(\"Validation data shape:\", len(validation_data))\n",
    "print(\"Testing data shape:\", len(testing_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
