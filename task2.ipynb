{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "457d0ae0-bb87-4238-a969-441f3de9120e",
   "metadata": {},
   "source": [
    "### Now try to explore the 995K FakeNewsCorpus subset Download 995K FakeNewsCorpus subset. Make at least three non-trivial observations/discoveries about the data. These observations could be related to outliers, artefacts, or even better: genuinely interesting patterns in the data that could potentially be used for fake-news detection. Examples of simple observations could be how many missing values there are in particular columns - or what the distribution over domains is. Be creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1afb5a60-2820-4ce9-aae0-d7644d2b3525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows processed: 995000\n",
      "\n",
      "Missing values in metadata columns (count):\n",
      "authors             442757\n",
      "meta_keywords        38790\n",
      "meta_description    525106\n",
      "tags                764081\n",
      "summary             995000\n",
      "dtype: int64\n",
      "\n",
      "Missing values in metadata columns (percentage):\n",
      "authors              44.50\n",
      "meta_keywords         3.90\n",
      "meta_description     52.77\n",
      "tags                 76.79\n",
      "summary             100.00\n",
      "dtype: float64\n",
      "\n",
      "Domain distribution (top 10):\n",
      "nytimes.com           176144\n",
      "beforeitsnews.com      91468\n",
      "dailykos.com           77640\n",
      "express.co.uk          55983\n",
      "nationalreview.com     37377\n",
      "sputniknews.com        37229\n",
      "abovetopsecret.com     27947\n",
      "wikileaks.org          23699\n",
      "www.newsmax.com        12688\n",
      "www.ammoland.com       11129\n",
      "dtype: int64\n",
      "\n",
      "Total articles with explicity 'error' in content: 78554\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"995,000_rows.csv\"\n",
    "chunksize = 10000 \n",
    "\n",
    "# Columns to analyze for missing metadata\n",
    "metadata_cols = ['authors', 'meta_keywords', 'meta_description', 'tags', 'summary']\n",
    "\n",
    "# Accumulators\n",
    "total_rows = 0\n",
    "missing_counts_acc = None\n",
    "domain_counts_acc = {}\n",
    "error_count_acc = 0\n",
    "content_lengths = []\n",
    "\n",
    "# Process the CSV in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize, low_memory=False):\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Observation 1: Missing values for metadata columns\n",
    "    chunk_missing = chunk[metadata_cols].isnull().sum()\n",
    "    if missing_counts_acc is None:\n",
    "        missing_counts_acc = chunk_missing\n",
    "    else:\n",
    "        missing_counts_acc += chunk_missing\n",
    "\n",
    "    # Observation 2: Domain distribution \n",
    "    chunk_domain_counts = chunk['domain'].value_counts()\n",
    "    for domain, count in chunk_domain_counts.items():\n",
    "        domain_counts_acc[domain] = domain_counts_acc.get(domain, 0) + count\n",
    "\n",
    "    # Observation 3: Content Artifacts and Anomalies\n",
    "    # Convert the 'content' column to string\n",
    "    chunk['content'] = chunk['content'].astype(str)\n",
    "    # Detect rows containing explicitly \"error\"\n",
    "    error_mask = chunk['content'].str.contains(r\"\\berror\\b\", case=False, regex=True, na=False)\n",
    "    error_count_acc += error_mask.sum()\n",
    "\n",
    "\n",
    "# Results after processing\n",
    "print(\"Total rows processed:\", total_rows)\n",
    "\n",
    "print(\"\\nMissing values in metadata columns (count):\")\n",
    "print(missing_counts_acc)\n",
    "print(\"\\nMissing values in metadata columns (percentage):\")\n",
    "print((missing_counts_acc / total_rows * 100).round(2))\n",
    "\n",
    "print(\"\\nDomain distribution (top 10):\")\n",
    "domain_series = pd.Series(domain_counts_acc).sort_values(ascending=False)\n",
    "print(domain_series.head(10))\n",
    "\n",
    "print(\"\\nTotal articles with explicity 'error' in content:\", error_count_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d77214-2dc5-43a2-a77e-8bcc1f368a7d",
   "metadata": {},
   "source": [
    "#### Describe how you ended up representing the FakeNewsCorpus dataset (for instance with a Pandas dataframe). Argue for why you chose this design\n",
    "\n",
    "We decided to represent the FakeNewsCorpus dataset using a Pandas DataFrame because the data was already in CSV format with columns like id, domain, content, and metadata. Using a DataFrame made it straightforward to load the entire table into memory, even though it has nearly a million rows, and provided an easy way to clean, filter, and analyze the data using built-in functions. This design was chosen because it simplifies the data tasks in this task: we could quickly compute statistics, check for missing values, and even visualize distributions without writing a lot of code. Even though our computers sometimes struggles with very large files and can handle different sizes, Pandas lets us process the data in chunks, so we could easily work on a subset of the data and then combine the results. So we chose Pandas, because it turns this messy CSV data into a table that is easier to understand and work with, and it has all the tools that we needed.\n",
    "\n",
    "\n",
    "#### Did you discover any inherent problems with the data while working with it?\n",
    "\n",
    "We initially attempted to load the entire dataset using pd.read_csv with low_memory=False to address the DtypeWarnings (especially in columns 0 and 1, which indicated mixed data types), but this approach overwhelmed our system and caused it to crash. We had to kill the process. Recognizing that our computer couldn’t handle the full dataset at once, we switched to a chunk-based approach. \n",
    "\n",
    "\n",
    "#### Report key properties of the data set - for instance through statistics or visualization\n",
    "\n",
    "Here we added some coding so we could see even more statistics.\n",
    "\n",
    "```python\n",
    "# Accumulate content lengths for later statistics\n",
    "content_lengths.extend(chunk['content'].apply(len).tolist())\n",
    "\n",
    "print(\"\\nContent length statistics:\")\n",
    "print(pd.Series(content_lengths).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4814b9ef-869b-477b-80ec-d97f3f0dea0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows processed: 995000\n",
      "\n",
      "Missing values in metadata columns (count):\n",
      "authors             442757\n",
      "meta_keywords        38790\n",
      "meta_description    525106\n",
      "tags                764081\n",
      "summary             995000\n",
      "dtype: int64\n",
      "\n",
      "Missing values in metadata columns (percentage):\n",
      "authors              44.50\n",
      "meta_keywords         3.90\n",
      "meta_description     52.77\n",
      "tags                 76.79\n",
      "summary             100.00\n",
      "dtype: float64\n",
      "\n",
      "Domain distribution (top 10):\n",
      "nytimes.com           176144\n",
      "beforeitsnews.com      91468\n",
      "dailykos.com           77640\n",
      "express.co.uk          55983\n",
      "nationalreview.com     37377\n",
      "sputniknews.com        37229\n",
      "abovetopsecret.com     27947\n",
      "wikileaks.org          23699\n",
      "www.newsmax.com        12688\n",
      "www.ammoland.com       11129\n",
      "dtype: int64\n",
      "\n",
      "Total articles with 'error' in content: 78554\n",
      "\n",
      "Content length statistics:\n",
      "count    995000.000000\n",
      "mean       2851.342791\n",
      "std        4111.355649\n",
      "min           3.000000\n",
      "25%         655.000000\n",
      "50%        1817.000000\n",
      "75%        3690.000000\n",
      "max      189025.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"995,000_rows.csv\"\n",
    "chunksize = 10000  \n",
    "\n",
    "# Columns to analyze for missing metadata\n",
    "metadata_cols = ['authors', 'meta_keywords', 'meta_description', 'tags', 'summary']\n",
    "\n",
    "# Accumulators\n",
    "total_rows = 0\n",
    "missing_counts_acc = None\n",
    "domain_counts_acc = {}\n",
    "error_count_acc = 0\n",
    "content_lengths = []\n",
    "\n",
    "# Process the CSV in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize, low_memory=False):\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Observation 1: Missing values for metadata columns \n",
    "    chunk_missing = chunk[metadata_cols].isnull().sum()\n",
    "    if missing_counts_acc is None:\n",
    "        missing_counts_acc = chunk_missing\n",
    "    else:\n",
    "        missing_counts_acc += chunk_missing\n",
    "\n",
    "    # Observation 2: Domain distribution \n",
    "    chunk_domain_counts = chunk['domain'].value_counts()\n",
    "    for domain, count in chunk_domain_counts.items():\n",
    "        domain_counts_acc[domain] = domain_counts_acc.get(domain, 0) + count\n",
    "\n",
    "    # Observation 3: Content Artifacts and Anomalies\n",
    "    # Convert the 'content' column to string\n",
    "    chunk['content'] = chunk['content'].astype(str)\n",
    "    # Detect rows containing \"error\"\n",
    "    error_mask = chunk['content'].str.contains(r\"\\berror\\b\", case=False, regex=True, na=False)\n",
    "    error_count_acc += error_mask.sum()\n",
    "\n",
    "    # Accumulate content lengths\n",
    "    content_lengths.extend(chunk['content'].apply(len).tolist())\n",
    "\n",
    "# Results after processing\n",
    "print(\"Total rows processed:\", total_rows)\n",
    "\n",
    "print(\"\\nMissing values in metadata columns (count):\")\n",
    "print(missing_counts_acc)\n",
    "print(\"\\nMissing values in metadata columns (percentage):\")\n",
    "print((missing_counts_acc / total_rows * 100).round(2))\n",
    "\n",
    "print(\"\\nDomain distribution (top 10):\")\n",
    "domain_series = pd.Series(domain_counts_acc).sort_values(ascending=False)\n",
    "print(domain_series.head(10))\n",
    "\n",
    "print(\"\\nTotal articles with 'error' in content:\", error_count_acc)\n",
    "\n",
    "print(\"\\nContent length statistics:\")\n",
    "print(pd.Series(content_lengths).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e383673-3dd3-483f-89e0-a80d6e7b4459",
   "metadata": {},
   "source": [
    "And from these value we can see that\n",
    "\n",
    "**Total rows processed**: 995000\n",
    "    This means the dataset has 995,000 individual articles (rows).\n",
    "\n",
    "Missing values in **metadata columns** (count and percentage):\n",
    "    For each metadata column, the script counted how many articles were missing data.\n",
    "\n",
    "**authors**: 442,757 articles (about 44.50%) are missing an author name.\n",
    "\n",
    "**meta_keywords**: 38,790 articles (3.90%) are missing meta keywords. (These are words or phrases included in a webpage’s HTML header intended to represent the main topics covered in the content)\n",
    "\n",
    "**meta_description**: 525,106 articles (52.77%) have no meta description. (This is a brief summary of the webpage’s content provided in the HTML header.)\n",
    "\n",
    "**tags**: 764,081 articles (76.79%) are missing tags. (Tags are labels or categories assigned to content by the publisher or content management system.)\n",
    "\n",
    "**summary**: 995,000 articles (100%) have no summary at all.\n",
    "In plain terms, many articles lack additional descriptive information, which might be a problem when trying to use this extra data.\n",
    "\n",
    "**Domain distribution** (top 10): This tells you how many articles come from each website. For example:\n",
    "    The New York Times (nytimes.com) appears 176,144 times.\n",
    "    BeforeItsNews.com appears 91,468 times.\n",
    "        And so on for the top 10 sites. This shows which sources are most common in the dataset.\n",
    "\n",
    "**Total articles** with 'error' in content: 78554\n",
    "    This means that 84 articles include the phrase \"Fatal error\" in their text. This might indicate issues in how the articles were scraped or technical errors on the source websites.\n",
    "\n",
    "**Content length statistics**: These numbers summarize how long the article texts are (in characters). For example:\n",
    "    **mean**: On average, each article has about 2,851 characters.\n",
    "    **std (standard deviation)**: There is a lot of variation (4,111 characters) in article lengths.\n",
    "    **min**: The shortest article has 3 characters.\n",
    "    **25%, 50%, 75%**: 25% of articles have 655 characters or fewer, the median (50%) is 1,817 characters, and 75% of articles have 3,690 characters or fewer.\n",
    "\n",
    "**max**: The longest article is 189,025 characters long.\n",
    "    This tells us that while most articles are a few thousand characters long, there are some extremely long outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
