{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Simple Logistic Regression Model (~1 page)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***You should create one or more reasonable baselines for your Fake News predictor. These should be simple models that you can use to benchmark your more advanced models against. You should aim to train a binary classification model that can predict whether an article is reliable or fake.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Briefly discuss how you grouped the labels into two groups. Are there any limitations that could arise from the decisions you made when grouping the labels?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Suppress UserWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Suppress FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)  # Suppress DtypeWarnings\n",
    "\n",
    "file_path = \"/Users/simonhvidtfeldt/Group_project/Part 1/cleaned_file.csv\"  # File to read\n",
    "column_name = \"type\"  # Column to search\n",
    "labels_to_search = [\"fake\", \"satire\", \"bias\", \"conspiracy\", \"state\", \"junksci\", \"hate\", \"clickbait\", \"unreliable\", \"political\", \"reliable\"]  # List of labels to search for\n",
    "\n",
    "print(\"Counting amount of specific labels in column:\")\n",
    "\n",
    "def count_column_values(file_path, column_name, labels_to_search):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\" - Read file\")\n",
    "\n",
    "    # Check if the column exists\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Error: Column '{column_name}' not found in the CSV file.\")\n",
    "        return\n",
    "\n",
    "    # Extract values from the column\n",
    "    values = df[column_name].dropna().astype(str)  # Convert to string & drop NaN values\n",
    "    print(\" - Extract values from the column\")\n",
    "\n",
    "    # Filter the values based on the labels_to_search list\n",
    "    filtered_values = [value for value in values if value in labels_to_search]\n",
    "    \n",
    "    # Count occurrences of the filtered values\n",
    "    value_counts = Counter(filtered_values)\n",
    "    print(\" - Count occurrences\")\n",
    "\n",
    "    print(\"\\nResult of function:\")\n",
    "    for value, count in value_counts.items():\n",
    "        print(f\" - {value}: \\t{count}\")\n",
    "\n",
    "    # Plotting the result\n",
    "    plot_data = pd.DataFrame(value_counts.items(), columns=[column_name, 'Count'])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=column_name, y='Count', data=plot_data, palette=\"deep\")\n",
    "    plt.xticks(rotation=45)  \n",
    "    plt.title(f\"Occurrences of {column_name}\")\n",
    "    plt.xlabel(f\"{column_name}\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()  \n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "count_column_values(file_path, column_name, labels_to_search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "FakeNews: 0, TrueNews: 1\n",
    "\n",
    "Fake News: Fake  \n",
    "Satire: Remove  \n",
    "Extreme Bias: Fake  \n",
    "Conspiracy Theory: Remove  \n",
    "State News: Remove  \n",
    "Junk Science: Remove  \n",
    "Hate News: Remove  \n",
    "Clickbait: True  \n",
    "Proceed with Caution: Remove  \n",
    "Political: True  \n",
    "Credible: True  \n",
    "\n",
    "---\n",
    "\n",
    "CATEGORY EXPLANATIONS\n",
    "\n",
    "Fake News:  \n",
    "Labeled as fake due to its misleading or intentionally deceptive nature.  \n",
    "\n",
    "Satire:  \n",
    "Satire is highly context-dependent and, when taken out of context, can lead to misunderstandings in both directions.  \n",
    "\n",
    "Extreme Bias:  \n",
    "Due to its reliance on highly questionable foundations, often associated with propaganda or fake news, extreme bias is classified as fake news.  \n",
    "\n",
    "Conspiracy Theory:  \n",
    "Conspiracy theories are based on non-scientific claims. While they may contain some true elements, they often mix verified information with misinformation, making them unreliable.  \n",
    "\n",
    "State News:  \n",
    "Excluded due to the lack of data or counts, making it an indifferent feature.  \n",
    "\n",
    "Junk Science:  \n",
    "Based on non-scientific theories that, in a binary classification system, would be considered fake. However, some theories may be difficult to prove scientifically. The non-traditional scientific methods used might overlook truths, such as in topics like spirituality.  \n",
    "\n",
    "Hate News:  \n",
    "(To be analyzed using `most.common` for further classification.)  \n",
    "\n",
    "Clickbait:  \n",
    "Clickbait often originates from credible sources but presents information in a misleading, exaggerated way. While it is not outright false, it angles the narrative in a way that can distort reality without technically spreading fake news.  \n",
    "\n",
    "Proceed with Caution:  \n",
    "This category contains both fake and potentially true content but requires further verification. By definition, it presents hypotheses that are not yet verified and therefore cannot be conclusively categorized as true or false.  \n",
    "\n",
    "Political:  \n",
    "Generally derived from verified sources of factual information but often framed to align with a particular political ideology. It does not cross the threshold into fake news.  \n",
    "\n",
    "Credible:  \n",
    "Adheres to ethical journalistic standards, ensuring thorough source verification before publication.  \n",
    "\n",
    "---\n",
    "\n",
    "Kig p√• `most.common` for Hate News.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count after removing non relevant types:\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Suppress UserWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Suppress FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)  # Suppress DtypeWarnings\n",
    "\n",
    "file_path = \"/Users/simonhvidtfeldt/Group_project/Part 1/cleaned_file.csv\"  # File to read\n",
    "column_name = \"type\"  # Column to search\n",
    "labels_to_search = [\"fake\", \"bias\", \"clickbait\", \"political\", \"reliable\"]  # List of labels to search for\n",
    "\n",
    "count_column_values(file_path, column_name, labels_to_search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now lets see how the distrubution of \"Fake\" and \"True\" looks like, when we add the count of each type together, for their respective category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Suppress UserWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Suppress FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)  # Suppress DtypeWarnings\n",
    "\n",
    "file_path = \"/Users/simonhvidtfeldt/Group_project/995,000_rows.csv\"  # File to read\n",
    "column_name = \"type\"  # Column to search\n",
    "\n",
    "# Function to categorize and sum counts based on labels\n",
    "def categorize_and_count(file_path, column_name):\n",
    "    # Define the categories and their corresponding labels\n",
    "    true_labels = [\"political\", \"reliable\", \"clickbait\"]\n",
    "    false_labels = [\"fake\", \"bias\"]\n",
    "    \n",
    "    # Read the file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract values from the column and drop NaN\n",
    "    values = df[column_name].dropna().astype(str)  # Convert to string & drop NaN values\n",
    "    \n",
    "    # Count occurrences of each label\n",
    "    value_counts = Counter(values)\n",
    "\n",
    "    # Calculate the sum of counts for True and False categories\n",
    "    true_count = sum([value_counts[label] for label in true_labels if label in value_counts])\n",
    "    false_count = sum([value_counts[label] for label in false_labels if label in value_counts])\n",
    "\n",
    "    # Print the counts for True and False categories\n",
    "    print(f\"True count (political, reliable): {true_count}\")\n",
    "    print(f\"False count (fake, bias): {false_count}\")\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    plot_data = pd.DataFrame({\"Category\": [\"True\", \"Fake\"], \"Count\": [true_count, false_count]})\n",
    "\n",
    "    # Plotting the result\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = sns.barplot(x=\"Category\", y=\"Count\", data=plot_data, palette=\"deep\")\n",
    "\n",
    "    # Add horizontal lines for every 100,000 count\n",
    "    y_max = max(true_count, false_count)  # Get max count\n",
    "    for y in np.arange(100000, y_max + 100000, 100000):\n",
    "        plt.axhline(y=y, color=\"gray\", linestyle=\"--\", linewidth=0.8, alpha=0.7)\n",
    "\n",
    "    # Add count values on top of each bar\n",
    "    for p in ax.patches:\n",
    "        ax.text(p.get_x() + p.get_width() / 2, p.get_height() - (y_max * 0.05), \n",
    "                f\"{int(p.get_height()):,}\", ha=\"center\", fontsize=12, color=\"black\")\n",
    "\n",
    "    # Labels and title\n",
    "    plt.title(\"Count of True vs Fake Categories\")\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: Call the function to categorize and plot the data\n",
    "categorize_and_count(file_path, column_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the way we have decided to categorize our data, we can see that we have a surplus of \"True\" articles in the data we want to analyze. We have to keep this in mind when training and evaluating our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Start by implementing and training a simple logistic regression classifier using a fixed vocabulary of the 10,000 most frequent words extracted from the content field, as the input features. You do not need to apply TF-IDF weighting to the features. It should take no more than five minutes to fit this model on a modern laptop, and you should expect to achieve an F1 score of ~94% on your test split. Write in your report the performance that you achieve with your implementation of this model, and remember to report any hyper-parameters used for the training process.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file /Users/simonhvidtfeldt/Group_project/Part 1/cleaned_file.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "CODE FROM PART_2_TASK_1\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)  # Suppress DtypeWarnings\n",
    "\n",
    "Final_file_path = \"/Users/simonhvidtfeldt/Group_project/Part 1/cleaned_file.csv\" #Insert Cleaned file here\n",
    "data = pd.read_csv(Final_file_path)\n",
    "print(f\"Loaded file '{Final_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Created the 'label' column\n",
      " - Converted 'True' to 1 and 'Fake' to 0\n",
      " - Dropped rows with NaN in the 'label' column\n",
      " - Saved the DataFrame to a CSV file called 'cleaned_file_with_label.csv'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "CODE FROM PART_2_TASK_1 *EDITED*\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Define mapping\n",
    "label_mapping = {\n",
    "    \"['polit']\": \"True\",\n",
    "    \"['clickbait']\": \"True\",\n",
    "    \"['reliabl']\": \"True\",\n",
    "    \"['fake']\": \"Fake\",\n",
    "    \"['bia']\": \"Fake\"\n",
    "}\n",
    "\n",
    "# Create the 'label' column\n",
    "data[\"label\"] = data[\"type\"].map(label_mapping)\n",
    "print(\" - Created the 'label' column\")\n",
    "\n",
    "# Convert 'True' to 1 and 'Fake' to 0\n",
    "data[\"label\"] = data[\"label\"].map({\"True\": 1, \"Fake\": 0})\n",
    "print(\" - Converted 'True' to 1 and 'Fake' to 0\")\n",
    "\n",
    "# Drop rows with NaN in the 'label' column\n",
    "data = data.dropna(subset=[\"label\"])\n",
    "print(\" - Dropped rows with NaN in the 'label' column\")\n",
    "\n",
    "# Save the DataFrame to a CSV file (if needed)\n",
    "data.to_csv(\"cleaned_file_with_label.csv\", index=False)\n",
    "print(f\" - Saved the DataFrame to a CSV file called 'cleaned_file_with_label.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Logistic regression for content:\n",
      "  - Loaded CSV.\n",
      "  - Ensured text data is string type.\n",
      "  - Vectorized text data.\n",
      "  - Used the binary 'label' column as the target variable.\n",
      "  - Splitting dataset into train, validation, and test sets.\n",
      "  - Implementing logistic regression.\n",
      "\n",
      "Results of logistic regression:\n",
      "  - F1-score: 0.9194\n",
      "\n",
      "Hyperparameters for Logistic Regression:\n",
      "  - Max Iterations: 500\n",
      "  - Solver: saga\n",
      "  - Random State: 42\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "CODE FROM PART_2_TASK_1 *EDITED*\n",
    "\n",
    "LOGISTIC REGRESSION FOR CONTENT \n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"Beginning Logistic regression for content:\")\n",
    "\n",
    "# Load CSV\n",
    "data = pd.read_csv(\"/Users/simonhvidtfeldt/DS-Fake-News-Project-1/Part 2/cleaned_file_with_label.csv\")\n",
    "print(\"  - Loaded CSV.\")\n",
    "\n",
    "# Ensure text data is string type\n",
    "data[\"content\"] = data[\"content\"].fillna(\"\").astype(str)\n",
    "print(\"  - Ensured text data is string type.\")\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer(max_features=10000, binary=True)\n",
    "X = vectorizer.fit_transform(data[\"content\"])\n",
    "print(\"  - Vectorized text data.\")\n",
    "\n",
    "# Use the binary 'label' column as the target variable\n",
    "y = data[\"label\"].astype(int)  # Ensure 'label' is integer type\n",
    "print(\"  - Used the binary 'label' column as the target variable.\")\n",
    "\n",
    "# Splitting dataset into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "print(\"  - Splitting dataset into train, validation, and test sets.\")\n",
    "\n",
    "# Implementing logistic regression\n",
    "clf = LogisticRegression(max_iter=500, random_state=42, solver=\"saga\", n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"  - Implementing logistic regression.\")\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average=\"binary\")  # Use 'binary' for binary classification\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults of logistic regression:\")\n",
    "print(f\"  - F1-score: {f1:.4f}\\n\")\n",
    "print(\"Hyperparameters for Logistic Regression:\")\n",
    "print(f\"  - Max Iterations: {clf.max_iter}\")\n",
    "print(f\"  - Solver: {clf.solver}\")\n",
    "print(f\"  - Random State: {clf.random_state}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Consider whether it would make sense to include meta-data features as well. If so, which ones, and why? If relevant, report the performance when including these additional features and compare it to the first baselines. Discuss whether these results match your expectations.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All meta data features except `meta_keywords` are missing more than half of their values. Therefor we will only try to incorparate this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Logistic Regression with meta data features:\n",
      "  - Loaded CSV.\n",
      "  - Ensured text data is string type and handled missing values.\n",
      "  - Combined all text columns into a single feature.\n",
      "  - Vectorized the combined text feature.\n",
      "  - Used the binary 'label' column as the target variable.\n",
      "  - Splitting dataset into train, validation, and test sets.\n",
      "  - Implementing logistic regression.\n",
      "\n",
      "Results of Logistic Regression with meta data features:\n",
      "  - F1-score: 0.9200\n",
      "\n",
      "Hyperparameters for Logistic Regression:\n",
      "  - Max Iterations: 500\n",
      "  - Solver: saga\n",
      "  - Random State: 42\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "CODE FROM PART_2_TASK_1 EDITED\n",
    "\n",
    "LOGISTIC REGRESSION FOR CONTENT + META_KEYWORDS\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"Beginning Logistic Regression with meta data features:\")\n",
    "\n",
    "# Load CSV\n",
    "data = pd.read_csv(\"/Users/simonhvidtfeldt/Group_project/cleaned_file_with_label.csv\")\n",
    "print(\"  - Loaded CSV.\")\n",
    "\n",
    "# Ensure text data is string type and handle missing values\n",
    "text_columns = [\"content\", \"meta_keywords\"]\n",
    "for col in text_columns:\n",
    "    data[col] = data[col].fillna(\"\").astype(str)\n",
    "print(\"  - Ensured text data is string type and handled missing values.\")\n",
    "\n",
    "# Combine all text columns into a single feature\n",
    "data[\"combined_text\"] = (\n",
    "    data[\"content\"] + \" \" +\n",
    "    data[\"meta_keywords\"] + \" \" \n",
    ")\n",
    "\n",
    "print(\"  - Combined all text columns into a single feature.\")\n",
    "\n",
    "# Vectorize the combined text feature\n",
    "vectorizer = TfidfVectorizer(max_features=10000, binary=True)\n",
    "X = vectorizer.fit_transform(data[\"combined_text\"])\n",
    "print(\"  - Vectorized the combined text feature.\")\n",
    "\n",
    "# Use the binary 'label' column as the target variable\n",
    "y = data[\"label\"].astype(int)  # Ensure 'label' is integer type\n",
    "print(\"  - Used the binary 'label' column as the target variable.\")\n",
    "\n",
    "# Splitting dataset into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "print(\"  - Splitting dataset into train, validation, and test sets.\")\n",
    "\n",
    "# Implementing logistic regression\n",
    "clf = LogisticRegression(max_iter=500, random_state=42, solver=\"saga\", n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"  - Implementing logistic regression.\")\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average=\"binary\")  # Use 'binary' for binary classification\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults of Logistic Regression with meta data features:\")\n",
    "print(f\"  - F1-score: {f1:.4f}\\n\")\n",
    "print(\"Hyperparameters for Logistic Regression:\")\n",
    "print(f\"  - Max Iterations: {clf.max_iter}\")\n",
    "print(f\"  - Solver: {clf.solver}\")\n",
    "print(f\"  - Random State: {clf.random_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Apply your data preprocessing pipeline to the extra reliable data you scraped during Graded Exercise 2 and add this to the training data and observe how this changes the performance of your simple model. Discuss whether you will continue to use this extra reliable data for the Advanced Model.***\n",
    "\n",
    "***For the remainder of the project, we will limit ourselves to main-text data only (i.e. no meta-data). This makes it easier to do the cross-domain experiment in Part 4 (which does not have the same set of meta-data fields).***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we apply our data preprocessing pipeline to the extra reliable data we scraped during Graded Exercise 2 and add this to the training data and observe how this changes the performance of our simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_file = \"/Users/simonhvidtfeldt/DS-Fake-News-Project-1/Part 2/output_big.txt\"\n",
    "output_file = \"/Users/simonhvidtfeldt/DS-Fake-News-Project-1/Part 2/output_big_matched.csv\"\n",
    "\n",
    "def convert_to_csv(input_file, output_file):\n",
    "    # Opens a text-file and reads all lines into a list\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Initializes a list to save articles and a dictionary to save metadata for the \"current article\"\n",
    "    articles = []\n",
    "    current_article = {}\n",
    "    current_key = None\n",
    "\n",
    "    # Checks each line. If empty, it means we've finished an article and the current article is added to articles\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if current_article:\n",
    "                articles.append(current_article)\n",
    "                current_article = {}\n",
    "                current_key = None\n",
    "        else:\n",
    "            # Check if line contains a \":\". If it does we create a key and a value. Example \"headline: \"Some headline\".\n",
    "            # If it doesn't we just add the line to the current article\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                current_article[key] = value\n",
    "                current_key = key\n",
    "            else:\n",
    "                if current_key and current_key in current_article:\n",
    "                    current_article[current_key] += ' ' + line\n",
    "\n",
    "    # Adds the last article to the list \"articles\"\n",
    "    if current_article:\n",
    "        articles.append(current_article)\n",
    "\n",
    "    # Defines the csv columm names\n",
    "    headers = [\n",
    "        '', 'id', 'domain', 'type', 'url', 'content', 'scraped_at',\n",
    "        'inserted_at', 'updated_at', 'title', 'authors', 'keywords',\n",
    "        'meta_keywords', 'meta_description', 'tags', 'summary', 'source', 'label'\n",
    "    ]\n",
    "\n",
    "    # Writes each article to a csv-file. The meta-data such as \"headline\", \"author\" and \"text\" is added to the relevent columns,\n",
    "    # The type is is set to reliable for all the articles. \n",
    "    # The label is set to 1 for all articles\n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)  \n",
    "\n",
    "        for article in articles:\n",
    "            row = [''] * len(headers)\n",
    "            row[headers.index('title')] = article.get('Headline', '')\n",
    "            row[headers.index('authors')] = article.get('Author', '')\n",
    "            row[headers.index('content')] = article.get('Text', '')\n",
    "            row[headers.index('type')] = 'reliable'                     # Set type to \"reliable\"\n",
    "            row[headers.index('label')] = 1                             # Set label to 1\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Calls the function, with the input file and the output file as arguments\n",
    "convert_to_csv(input_file,output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have converted the scraped article from bbc into the same format. Furthermore we have also added their type, which is reliable, and set their label to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/simonhvidtfeldt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/simonhvidtfeldt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1\n",
      "All done! Cleaned file saved as cleaned_file_FOR_SCRAPED.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define file path and chunk size\n",
    "file_path = \"/Users/simonhvidtfeldt/DS-Fake-News-Project-1/Part 2/output_big_matched.csv\"\n",
    "chunksize = 25000\n",
    "\n",
    "# Define your cleaning function\n",
    "def clean_text(data):\n",
    "    if not isinstance(data, str):  # Handle NaN values safely\n",
    "        return \"\"\n",
    "    data = data.lower()\n",
    "    data = re.sub(r'\\s+', \" \", data)\n",
    "    data = re.sub(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', \"<DATE>\", data)\n",
    "    data = re.sub(r'(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec).? \\d{1,2},? \\d{4}', \"<DATE>\", data)\n",
    "    data = re.sub(r'\\d{4}-\\d{2}-\\d{2}', \"<DATE>\", data)\n",
    "    data = re.sub(r'[\\w._%+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', \"<EMAIL>\", data)\n",
    "    data = re.sub(r'http[s]?://[^\\s]+', \"<URL>\", data)\n",
    "    data = re.sub(r'\\d+(\\.\\d+)?', \"<NUM>\", data)\n",
    "    return data\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Define function to tokenize, remove stopwords, and stem\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [ps.stem(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# List of columns to clean (with clean_text)\n",
    "columns_to_clean = [\n",
    "    \"id\", \"domain\", \"type\", \"url\", \"content\", \"title\", \"authors\", \"keywords\", \n",
    "    \"meta_keywords\", \"meta_description\", \"tags\", \"summary\"\n",
    "]\n",
    "\n",
    "# Create empty DataFrame to collect processed chunks\n",
    "preprocessed_data = []\n",
    "\n",
    "# Process chunks\n",
    "for chunk_number, chunk in enumerate(pd.read_csv(file_path, chunksize=chunksize, low_memory=False)):\n",
    "    # Apply cleaning function to specified columns\n",
    "    for col in columns_to_clean:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = chunk[col].apply(clean_text)\n",
    "    \n",
    "    # Remove stopwords, tokenize, and stem only for 'content' column\n",
    "    if 'content' in chunk.columns:\n",
    "        chunk['content'] = chunk['content'].astype(str).apply(tokenize_and_stem)\n",
    "    \n",
    "    preprocessed_data.append(chunk)\n",
    "    print(f\"Processed chunk {chunk_number + 1}\")\n",
    "\n",
    "# Combine and save all cleaned data\n",
    "final_df = pd.concat(preprocessed_data, ignore_index=True)\n",
    "final_df.to_csv(\"cleaned_file_FOR_SCRAPED.csv\", index=False)\n",
    "print(\"All done! Cleaned file saved as cleaned_file_FOR_SCRAPED.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have cleaned the file we can add it to the 995.000 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "\n",
    "scraped_And_cleaned_with_label = \"/Users/simonhvidtfeldt/DS-Fake-News-Project-1/Part 2/cleaned_file_FOR_SCRAPED.csv\"\n",
    "cleaned_file_with_label = \"/Users/simonhvidtfeldt/DS-Fake-News-Project-1/Part 2/cleaned_file_with_label.csv\" \n",
    "output = \"/Users/simonhvidtfeldt/DS-Fake-News-Project-1/Part 2/cleaned_file_plus_scraped.csv\"\n",
    "\n",
    "def add_csv_files(file1, file2, output_file):\n",
    "    # Increase the field size limit\n",
    "    max_int = sys.maxsize\n",
    "    while True:\n",
    "        try:\n",
    "            csv.field_size_limit(max_int)\n",
    "            break\n",
    "        except OverflowError:\n",
    "            max_int = int(max_int / 10)\n",
    "\n",
    "    # Read the first CSV file\n",
    "    with open(file1, 'r') as f1:\n",
    "        reader = csv.reader(f1)\n",
    "        header = next(reader)  # Read the header\n",
    "        rows_file1 = list(reader)  # Read all rows\n",
    "\n",
    "    # Read the second CSV file\n",
    "    with open(file2, 'r') as f2:\n",
    "        reader = csv.reader(f2)\n",
    "        next(reader)  # Skip the header since it is the same as file1\n",
    "        rows_file2 = list(reader)  # Read all rows\n",
    "\n",
    "    # Combine rows from both files\n",
    "    combined_rows = rows_file1 + rows_file2\n",
    "\n",
    "    # Write the combined rows to the output file\n",
    "    with open(output_file, 'w', newline='') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow(header)  # Write the header\n",
    "        writer.writerows(combined_rows)  # Write all rows\n",
    "\n",
    "# Example usage\n",
    "add_csv_files(scraped_And_cleaned_with_label,           \n",
    "                  cleaned_file_with_label,   \n",
    "                    output)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check, if the extra articles have been added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in cleaned_file_with_label.csv: 798\n",
      "Number of rows in scraped_plus_995k.csv: 995798\n",
      "Difference in rows: 995000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "file1 = \"/Users/simonhvidtfeldt/DS-Fake-News-Project-1/Part 2/cleaned_file_FOR_SCRAPED.csv\"\n",
    "file2 = \"/Users/simonhvidtfeldt/DS-Fake-News-Project-1/Part 2/cleaned_file_plus_scraped.csv\"\n",
    "\n",
    "# Load the CSV files and count rows\n",
    "def count_rows(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return len(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Count rows in both files\n",
    "rows_file1 = count_rows(file1)\n",
    "rows_file2 = count_rows(file2)\n",
    "\n",
    "# Calculate the difference\n",
    "difference = abs(rows_file1 - rows_file2)\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of rows in cleaned_file_with_label.csv: {rows_file1}\")\n",
    "print(f\"Number of rows in scraped_plus_995k.csv: {rows_file2}\")\n",
    "print(f\"Difference in rows: {difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen by, we have succesfully added the additional 798 articles to the cleaned_file_with_label. Great!\n",
    "now lets try to run the combined file through the simple logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Logistic regression for 995k + scraped:\n",
      "  - Loaded CSV.\n",
      "  - Dropped rows with missing values and converted label to int.\n",
      "  - Ensured text data is string type.\n",
      "  - Vectorized text data.\n",
      "  - Used the binary 'label' column as the target variable.\n",
      "  - Splitting dataset into train, validation, and test sets.\n",
      "  - Implementing logistic regression.\n",
      "\n",
      "Results of logistic regression of 995k + scraped:\n",
      "  - F1-score: 0.9180\n",
      "\n",
      "Hyperparameters for Logistic Regression:\n",
      "  - Max Iterations: 500\n",
      "  - Solver: saga\n",
      "  - Random State: 42\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "LOGISTIC REGRESSION FOR 995k + scraped \n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"Beginning Logistic regression for 995k + scraped:\")\n",
    "\n",
    "# Load CSV\n",
    "data = pd.read_csv(\"/Users/simonhvidtfeldt/DS-Fake-News-Project-1/Part 2/cleaned_file_plus_scraped.csv\")\n",
    "print(\"  - Loaded CSV.\")\n",
    "\n",
    "data = data.dropna(subset=[\"label\"])\n",
    "data[\"label\"] = data[\"label\"].astype(int)\n",
    "print(\"  - Dropped rows with missing values and converted label to int.\")\n",
    "\n",
    "# Ensure text data is string type\n",
    "data[\"content\"] = data[\"content\"].fillna(\"\").astype(str)\n",
    "print(\"  - Ensured text data is string type.\")\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer(max_features=10000, binary=True)\n",
    "X = vectorizer.fit_transform(data[\"content\"])\n",
    "print(\"  - Vectorized text data.\")\n",
    "\n",
    "# Use the binary 'label' column as the target variable\n",
    "y = data[\"label\"].astype(int)  # Ensure 'label' is integer type\n",
    "print(\"  - Used the binary 'label' column as the target variable.\")\n",
    "\n",
    "# Splitting dataset into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "print(\"  - Splitting dataset into train, validation, and test sets.\")\n",
    "\n",
    "# Implementing logistic regression\n",
    "clf = LogisticRegression(max_iter=500, random_state=42, solver=\"saga\", n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"  - Implementing logistic regression.\")\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average=\"binary\")  # Use 'binary' for binary classification\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults of logistic regression of 995k + scraped:\")\n",
    "print(f\"  - F1-score: {f1:.4f}\\n\")\n",
    "print(\"Hyperparameters for Logistic Regression:\")\n",
    "print(f\"  - Max Iterations: {clf.max_iter}\")\n",
    "print(f\"  - Solver: {clf.solver}\")\n",
    "print(f\"  - Random State: {clf.random_state}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm. It got worse. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
