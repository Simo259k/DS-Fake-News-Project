{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/simonhvidtfeldt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/simonhvidtfeldt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Setup complete. All modules are ready.\n"
     ]
    }
   ],
   "source": [
    "%run /Users/simonhvidtfeldt/Group_project/Setup.py\n",
    "# Det her kan i ignore eller lade være med at køre. Det er bare så jeg er sikker på at have installeret alt der skal bruges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "***Consider whether it would make sense to include meta-data features as well. If so, which ones, and why? If relevant, report the performance when including these additional features and compare it to the first baselines. Discuss whether these results match your expectations.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Task 1 we were asked to train a simple logistic regression classifier using a fixed vocabulary of the 10.000 most frequently used words, but we have access to many different meta-data features as well. These are \n",
    "\n",
    "-   id\n",
    "-   domain\n",
    "-   type\n",
    "-   url\n",
    "-   content\n",
    "-   scraped_at\n",
    "-   inserted_at\n",
    "-   updated_at\n",
    "-   title\n",
    "-   authors\n",
    "-   keywords\n",
    "-   meta_keywords\n",
    "-   meta_description\n",
    "-   tags\n",
    "-   summary\n",
    "\n",
    "Some of these are more interessting then others. \"url\" and \"id\" isnt very interesting in our simple logistic regression classifier. The \"url\" may include the domain, but the this can also be extracted from the \"domain\" feature. So including the actual \"url\" will just introduce unneseccary noice. Additionally, the \"id\" is just a identifier without any informative value. \n",
    "\n",
    "\"scraped_at\", \"inserted_at\" and \"updated_at\" are also not very interesting in it self, but could prove useful if we want to pull the timepatterens from the data.\n",
    "\n",
    "\"authors\" is a bit tricky. If a author is known to publish fake news, then this could be a good indicator. But if we have a lot of different authors we just risk overfitting our model, therefor we want include it. \n",
    "\n",
    "Now, we get to the interesting parts. \"domain\" will most likely have a pattern. Some domains are more likely to publish fake-news then others, and a simple logistic regression classifier could be trained to predict this. Another interesting mete-feature is the \"title\". A title will often include words that are indicative of the content of the article, and many click-bait titles are often use the same pattern with a catching title. Whith this being said, we actually cleaned out this during cleaning, so we wont be able to extract any reliable data.\n",
    "\n",
    "Similary to \"title\" is \"meta_description\" and \"summary\". These are often like the titles, but include more information about the content of the article.\n",
    "\n",
    "\"keywords\" and \"meta_keywords\" can be very informative. These will typically include good signals about the content of the article, and will prove usefull in our classifier.\n",
    "\n",
    "Lastly, \"tags\" is similar to \"keywords\" and \"meta_keywords\", and are therefor in a similar position. They will include useful information regarding the article, and is therefor also very useful for our classifier.\n",
    "\n",
    "So we will try to include the following meta-data features:\n",
    "\n",
    "-   meta_description\n",
    "-   content\n",
    "-   keywords\n",
    "-   meta_keywords\n",
    "-   tags\n",
    "-   title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "CODE FROM PART_2_TASK_1\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "Final_file_path = \"/Users/simonhvidtfeldt/Group_project/cleaned_file.csv\"\n",
    "data = pd.read_csv(Final_file_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the 'label' column\n",
      "Converted 'True' to 1 and 'Fake' to 0\n",
      "Dropped rows with NaN in the 'label' column\n",
      "Saved the DataFrame to a CSV file\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "CODE FROM PART_2_TASK_1 *EDITED*\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# Define mapping\n",
    "label_mapping = {\n",
    "    \"['polit']\": \"True\",\n",
    "    \"['clickbait']\": \"True\",\n",
    "    \"['reliabl']\": \"True\",\n",
    "    \"['fake']\": \"Fake\",\n",
    "    \"['bia']\": \"Fake\"\n",
    "}\n",
    "\n",
    "# Create the 'label' column\n",
    "data[\"label\"] = data[\"type\"].map(label_mapping)\n",
    "print(\" - Created the 'label' column\")\n",
    "\n",
    "# Convert 'True' to 1 and 'Fake' to 0\n",
    "data[\"label\"] = data[\"label\"].map({\"True\": 1, \"Fake\": 0})\n",
    "print(\" - Converted 'True' to 1 and 'Fake' to 0\")\n",
    "\n",
    "# Drop rows with NaN in the 'label' column (if any)\n",
    "data = data.dropna(subset=[\"label\"])\n",
    "print(\" - Dropped rows with NaN in the 'label' column\")\n",
    "\n",
    "# Save the DataFrame to a CSV file (if needed)\n",
    "data.to_csv(\"cleaned_file_with_label.csv\", index=False)\n",
    "print(\" - Saved the DataFrame to a CSV file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Logistic regression:\n",
      "    Loaded CSV.\n",
      "    Ensured text data is string type.\n",
      "    Vectorized text data.\n",
      "    Used the binary 'label' column as the target variable.\n",
      "    Splitting dataset into train, validation, and test sets.\n",
      "    Implementing logistic regression.\n",
      "\n",
      "Results of logistic regression:\n",
      "    F1-score: 0.9194\n",
      "\n",
      "Hyperparameters for Logistic Regression:\n",
      "    Max Iterations: 500\n",
      "    Solver: saga\n",
      "    Random State: 42\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "CODE FROM PART_2_TASK_1 *EDITED*\n",
    "\n",
    "LOGISTIC REGRESSION FOR CONTENT \n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "print(\"Beginning Logistic regression for content:\")\n",
    "\n",
    "# Load CSV\n",
    "data = pd.read_csv(\"/Users/simonhvidtfeldt/Group_project/cleaned_file_with_label.csv\")\n",
    "print(\"  - Loaded CSV.\")\n",
    "\n",
    "# Ensure text data is string type\n",
    "data[\"content\"] = data[\"content\"].fillna(\"\").astype(str)\n",
    "print(\"  - Ensured text data is string type.\")\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer(max_features=10000, binary=True)\n",
    "X = vectorizer.fit_transform(data[\"content\"])\n",
    "print(\"  - Vectorized text data.\")\n",
    "\n",
    "# Use the binary 'label' column as the target variable\n",
    "y = data[\"label\"].astype(int)  # Ensure 'label' is integer type\n",
    "print(\"  - Used the binary 'label' column as the target variable.\")\n",
    "\n",
    "# Splitting dataset into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "print(\"  - Splitting dataset into train, validation, and test sets.\")\n",
    "\n",
    "# Implementing logistic regression\n",
    "clf = LogisticRegression(max_iter=500, random_state=42, solver=\"saga\", n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"  - Implementing logistic regression.\")\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average=\"binary\")  # Use 'binary' for binary classification\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults of logistic regression:\")\n",
    "print(f\"  - F1-score: {f1:.4f}\\n\")\n",
    "print(\"Hyperparameters for Logistic Regression:\")\n",
    "print(f\"  - Max Iterations: {clf.max_iter}\")\n",
    "print(f\"  - Solver: {clf.solver}\")\n",
    "print(f\"  - Random State: {clf.random_state}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Logistic Regression with meta data features:\n",
      "  - Loaded CSV.\n",
      "  - Ensured text data is string type and handled missing values.\n",
      "  - Combined all text columns into a single feature.\n",
      "  - Vectorized the combined text feature.\n",
      "  - Used the binary 'label' column as the target variable.\n",
      "  - Splitting dataset into train, validation, and test sets.\n",
      "  - Implementing logistic regression.\n",
      "\n",
      "Results of Logistic Regression with Additional Columns:\n",
      "  - F1-score: 0.9296\n",
      "\n",
      "Hyperparameters for Logistic Regression:\n",
      "  - Max Iterations: 500\n",
      "  - Solver: saga\n",
      "  - Random State: 42\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "CODE FROM PART_2_TASK_1 EDITED\n",
    "\n",
    "LOGISTIC REGRESSION FOR CONTENT + META DATA\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "print(\"Beginning Logistic Regression with meta data features:\")\n",
    "\n",
    "# Load CSV\n",
    "data = pd.read_csv(\"/Users/simonhvidtfeldt/Group_project/cleaned_file_with_label.csv\")\n",
    "print(\"  - Loaded CSV.\")\n",
    "\n",
    "# Ensure text data is string type and handle missing values\n",
    "text_columns = [\"content\", \"title\", \"meta_keywords\", \"meta_description\", \"tags\"]\n",
    "for col in text_columns:\n",
    "    data[col] = data[col].fillna(\"\").astype(str)\n",
    "print(\"  - Ensured text data is string type and handled missing values.\")\n",
    "\n",
    "# Combine all text columns into a single feature\n",
    "data[\"combined_text\"] = (\n",
    "    data[\"content\"] + \" \" +\n",
    "    data[\"title\"] + \" \" +\n",
    "    data[\"meta_keywords\"] + \" \" +\n",
    "    data[\"meta_description\"] + \" \" +\n",
    "    data[\"tags\"]\n",
    ")\n",
    "print(\"  - Combined all text columns into a single feature.\")\n",
    "\n",
    "# Vectorize the combined text feature\n",
    "vectorizer = TfidfVectorizer(max_features=10000, binary=True)\n",
    "X = vectorizer.fit_transform(data[\"combined_text\"])\n",
    "print(\"  - Vectorized the combined text feature.\")\n",
    "\n",
    "# Use the binary 'label' column as the target variable\n",
    "y = data[\"label\"].astype(int)  # Ensure 'label' is integer type\n",
    "print(\"  - Used the binary 'label' column as the target variable.\")\n",
    "\n",
    "# Splitting dataset into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "print(\"  - Splitting dataset into train, validation, and test sets.\")\n",
    "\n",
    "# Implementing logistic regression\n",
    "clf = LogisticRegression(max_iter=500, random_state=42, solver=\"saga\", n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"  - Implementing logistic regression.\")\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average=\"binary\")  # Use 'binary' for binary classification\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults of Logistic Regression with meta data features:\")\n",
    "print(f\"  - F1-score: {f1:.4f}\\n\")\n",
    "print(\"Hyperparameters for Logistic Regression:\")\n",
    "print(f\"  - Max Iterations: {clf.max_iter}\")\n",
    "print(f\"  - Solver: {clf.solver}\")\n",
    "print(f\"  - Random State: {clf.random_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the additional meta data features we where able to improve our F1 score from 0.9194 to 0.9296. This is as expected. By adding these additional features we are able to capture more information about the data and therefor improve the accuracy of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "***Apply your data preprocessing pipeline to the extra reliable data you scraped during Graded Exercise 2 and add this to the training data and observe how this changes the performance of your simple model. Discuss whether you will continue to use this extra reliable data for the Advanced Model.***\n",
    "\n",
    "***For the remainder of the project, we will limit ourselves to main-text data only (i.e. no meta-data). This makes it easier to do the cross-domain experiment in Part 4 (which does not have the same set of meta-data fields).***\n",
    "\n",
    "In this task we apply our data preprocessing pipeline to the extra reliable data we scraped during Graded Exercise 2 and add this to the training data and observe how this changes the performance of our simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def convert_to_csv(input_file, output_file):\n",
    "    # Opens a text-file and reads all lines into a list\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Initializes a list to save articles and a dictionary to save metadata for the \"current article\"\n",
    "    articles = []\n",
    "    current_article = {}\n",
    "    current_key = None\n",
    "\n",
    "    # Checks each line. If empty, it means we've finished an article and the current article is added to articles\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if current_article:\n",
    "                articles.append(current_article)\n",
    "                current_article = {}\n",
    "                current_key = None\n",
    "        else:\n",
    "            # Check if line contains a \":\". If it does we create a key and a value. Example \"headline: \"Some headline\".\n",
    "            # If it doesn't we just add the line to the current article\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                current_article[key] = value\n",
    "                current_key = key\n",
    "            else:\n",
    "                if current_key and current_key in current_article:\n",
    "                    current_article[current_key] += ' ' + line\n",
    "\n",
    "    # Adds the last article to the list \"articles\"\n",
    "    if current_article:\n",
    "        articles.append(current_article)\n",
    "\n",
    "    # Defines the csv columm names\n",
    "    headers = [\n",
    "        '', 'id', 'domain', 'type', 'url', 'content', 'scraped_at',\n",
    "        'inserted_at', 'updated_at', 'title', 'authors', 'keywords',\n",
    "        'meta_keywords', 'meta_description', 'tags', 'summary', 'source', 'label'\n",
    "    ]\n",
    "\n",
    "    # Writes each article to a csv-file. The meta-data such as \"headline\", \"author\" and \"text\" is added to the relevent columns,\n",
    "    # The type is is set to reliable for all the articles. \n",
    "    # The label is set to 1 for all articles\n",
    "    with open(output_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)  \n",
    "\n",
    "        for article in articles:\n",
    "            row = [''] * len(headers)\n",
    "            row[headers.index('title')] = article.get('Headline', '')\n",
    "            row[headers.index('authors')] = article.get('Author', '')\n",
    "            row[headers.index('content')] = article.get('Text', '')\n",
    "            row[headers.index('type')] = 'reliable'                     # Set type to \"reliable\"\n",
    "            row[headers.index('label')] = 1                             # Set label to 1\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Calls the function, with the input file and the output file as arguments\n",
    "convert_to_csv('/Users/simonhvidtfeldt/Group_project/output_big.txt',\n",
    "                '/Users/simonhvidtfeldt/Group_project/output_big_matched.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have converted the scraped article from bbc into the same format. Furthermore we have also added their type, which is reliable, and set their label to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/simonhvidtfeldt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/simonhvidtfeldt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define file path and chunk size\n",
    "file_path = \"/Users/simonhvidtfeldt/Group_project/output_big_matched.csv\"\n",
    "chunksize = 25000\n",
    "\n",
    "# Define your cleaning function\n",
    "def clean_text(data):\n",
    "    if not isinstance(data, str):  # Handle NaN values safely\n",
    "        return \"\"\n",
    "    data = data.lower()\n",
    "    data = re.sub(r'\\s+', \" \", data)\n",
    "    data = re.sub(r'\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}', \"<DATE>\", data)\n",
    "    data = re.sub(r'(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec).? \\d{1,2},? \\d{4}', \"<DATE>\", data)\n",
    "    data = re.sub(r'\\d{4}-\\d{2}-\\d{2}', \"<DATE>\", data)\n",
    "    data = re.sub(r'[\\w._%+-]+@[\\w.-]+\\.[a-zA-Z]{2,}', \"<EMAIL>\", data)\n",
    "    data = re.sub(r'http[s]?://[^\\s]+', \"<URL>\", data)\n",
    "    data = re.sub(r'\\d+(\\.\\d+)?', \"<NUM>\", data)\n",
    "    return data\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Define function to tokenize, remove stopwords, and stem\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [ps.stem(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# List of columns to process\n",
    "columns_to_clean = [\"Unamed: 0\",\n",
    "    \"id\", \"domain\", \"type\", \"url\", \"content\", \"scraped_at\", \"inserted_at\",\n",
    "    \"updated_at\", \"title\", \"authors\", \"keywords\", \"meta_keywords\",\n",
    "    \"meta_description\", \"tags\", \"summary\"\n",
    "]\n",
    "\n",
    "#Process and store all chunks in a single DataFrame\n",
    "preprocessed_data = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize, low_memory=False):\n",
    "    for col in columns_to_clean:\n",
    "        if col in chunk.columns:\n",
    "            # Clean\n",
    "            chunk[col] = chunk[col].apply(clean_text)\n",
    "            #Remove stopwords\n",
    "            chunk[col] = chunk[col].astype(str).apply(\n",
    "                lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words])\n",
    "            )\n",
    "            #Tokenize and stem\n",
    "            chunk[col] = chunk[col].apply(tokenize_and_stem)\n",
    "    # Append chunk to final DataFrame\n",
    "    preprocessed_data = pd.concat([preprocessed_data, chunk], ignore_index=True)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "preprocessed_data.to_csv(\"output_big_matched_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have cleaned the file we can add it to the 995.000 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "\n",
    "def add_csv_files(file1, file2, output_file):\n",
    "    # Increase the field size limit\n",
    "    max_int = sys.maxsize\n",
    "    while True:\n",
    "        try:\n",
    "            csv.field_size_limit(max_int)\n",
    "            break\n",
    "        except OverflowError:\n",
    "            max_int = int(max_int / 10)\n",
    "\n",
    "    # Read the first CSV file\n",
    "    with open(file1, 'r') as f1:\n",
    "        reader = csv.reader(f1)\n",
    "        header = next(reader)  # Read the header\n",
    "        rows_file1 = list(reader)  # Read all rows\n",
    "\n",
    "    # Read the second CSV file\n",
    "    with open(file2, 'r') as f2:\n",
    "        reader = csv.reader(f2)\n",
    "        next(reader)  # Skip the header since it is the same as file1\n",
    "        rows_file2 = list(reader)  # Read all rows\n",
    "\n",
    "    # Combine rows from both files\n",
    "    combined_rows = rows_file1 + rows_file2\n",
    "\n",
    "    # Write the combined rows to the output file\n",
    "    with open(output_file, 'w', newline='') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow(header)  # Write the header\n",
    "        writer.writerows(combined_rows)  # Write all rows\n",
    "\n",
    "# Example usage\n",
    "add_csv_files('/Users/simonhvidtfeldt/Group_project/cleaned_file_with_label.csv',           # file1\n",
    "                  \"/Users/simonhvidtfeldt/Group_project/output_big_matched_cleaned.csv\",    # file2\n",
    "                    '/Users/simonhvidtfeldt/Group_project/scraped_plus_995k.csv')           # The file we want to created, where they have been put together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in cleaned_file_with_label.csv: 678609\n",
      "Number of rows in scraped_plus_995k.csv: 679407\n",
      "Difference in rows: 798\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "file1 = \"/Users/simonhvidtfeldt/Group_project/cleaned_file_with_label.csv\"\n",
    "file2 = \"/Users/simonhvidtfeldt/Group_project/scraped_plus_995k.csv\"\n",
    "\n",
    "# Load the CSV files and count rows\n",
    "def count_rows(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return len(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Count rows in both files\n",
    "rows_file1 = count_rows(file1)\n",
    "rows_file2 = count_rows(file2)\n",
    "\n",
    "# Calculate the difference\n",
    "difference = abs(rows_file1 - rows_file2)\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of rows in cleaned_file_with_label.csv: {rows_file1}\")\n",
    "print(f\"Number of rows in scraped_plus_995k.csv: {rows_file2}\")\n",
    "print(f\"Difference in rows: {difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen by, we have succesfully added the additional 798 articles to the 995k rows. Great!\n",
    "now lets try to run the combined file through the simple logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Logistic regression for 995k + scraped:\n",
      "  - Loaded CSV.\n",
      "  - Ensured text data is string type.\n",
      "  - Vectorized text data.\n",
      "  - Used the binary 'label' column as the target variable.\n",
      "  - Splitting dataset into train, validation, and test sets.\n",
      "  - Implementing logistic regression.\n",
      "\n",
      "Results of logistic regression of 995k + scraped:\n",
      "  - F1-score: 0.9198\n",
      "\n",
      "Hyperparameters for Logistic Regression:\n",
      "  - Max Iterations: 500\n",
      "  - Solver: saga\n",
      "  - Random State: 42\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "CODE FROM PART_2_TASK_1 *EDITED*\n",
    "\n",
    "LOGISTIC REGRESSION FOR 995k + scraped \n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"Beginning Logistic regression for 995k + scraped:\")\n",
    "\n",
    "# Load CSV\n",
    "data = pd.read_csv(\"/Users/simonhvidtfeldt/Group_project/scraped_plus_995k.csv\")\n",
    "print(\"  - Loaded CSV.\")\n",
    "\n",
    "# Ensure text data is string type\n",
    "data[\"content\"] = data[\"content\"].fillna(\"\").astype(str)\n",
    "print(\"  - Ensured text data is string type.\")\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer(max_features=10000, binary=True)\n",
    "X = vectorizer.fit_transform(data[\"content\"])\n",
    "print(\"  - Vectorized text data.\")\n",
    "\n",
    "# Use the binary 'label' column as the target variable\n",
    "y = data[\"label\"].astype(int)  # Ensure 'label' is integer type\n",
    "print(\"  - Used the binary 'label' column as the target variable.\")\n",
    "\n",
    "# Splitting dataset into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "print(\"  - Splitting dataset into train, validation, and test sets.\")\n",
    "\n",
    "# Implementing logistic regression\n",
    "clf = LogisticRegression(max_iter=500, random_state=42, solver=\"saga\", n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"  - Implementing logistic regression.\")\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = clf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average=\"binary\")  # Use 'binary' for binary classification\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults of logistic regression of 995k + scraped:\")\n",
    "print(f\"  - F1-score: {f1:.4f}\\n\")\n",
    "print(\"Hyperparameters for Logistic Regression:\")\n",
    "print(f\"  - Max Iterations: {clf.max_iter}\")\n",
    "print(f\"  - Solver: {clf.solver}\")\n",
    "print(f\"  - Random State: {clf.random_state}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For part 2, task 1 we got an F1 score of 0.9194. By adding the extra 799 articles we were able to increase that score to 0.9198. Lets calculate the increase ine percentage. \n",
    "\n",
    "*Subtract the old F1 score from the new F1 score:*\n",
    "\n",
    "$$0.9198 - 0.9194 = 0.0004$$\n",
    "\n",
    "*Divide the difference by the old F1 score:*\n",
    "\n",
    "$$\\frac{0.0004}{0.9194}≈0.000435$$\n",
    "\n",
    "*Multiply by 100 to get the percentage increase:*\n",
    "\n",
    "$$0.000435 \\cdot 100 = 0.0435\\%$$\n",
    "\n",
    "So, the F1 score increased by 0.0435%. This is a very small increase. Either, the extra articles didn't provide significant new information, or the model is already very good at classifying the articles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
